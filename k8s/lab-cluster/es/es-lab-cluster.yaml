---
eck-elasticsearch:
spec:
  http:
    tls:
      selfSignedCertificate:
        disabled: true
  version: 9.0.2
  nodeSets:
  - name: masters
    count: 3
    config:
      node.roles: ["master"]
      # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
      # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
      # and leave node.store.allow_mmap unset.
      # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
      #
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 8Gi
              cpu: 2
        # Affinity/Anti-affinity settings for controlling the 'spreading' of Elasticsearch
        # pods across existing hosts.
        # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
        # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html#k8s-affinity-options
        #
        # affinity:
        #   nodeAffinity:
        #     requiredDuringSchedulingIgnoredDuringExecution:
        #       nodeSelectorTerms:
        #       - matchExpressions:
        #         - key: beta.kubernetes.io/instance-type
        #           operator: In
        #           # This should be adjusted to the instance type according to your setup
        #           #
        #           values:
        #           - highio
    # Volume Claim settings.
    # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-volume-claim-templates.html
    #
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 1Ti
        # Adjust to your storage class name
        #
        # storageClassName: local-storage
  - name: hot
    count: 3
    config:
      node.roles: ["data_hot", "data_content", "ingest"]
      # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
      # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
      # and leave node.store.allow_mmap unset.
      # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
      #
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 16Gi
              cpu: 4
        # Affinity/Anti-affinity settings for controlling the 'spreading' of Elasticsearch
        # pods across existing hosts.
        # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
        # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html#k8s-affinity-options
        #
        # affinity:
        #   nodeAffinity:
        #     requiredDuringSchedulingIgnoredDuringExecution:
        #       nodeSelectorTerms:
        #       - matchExpressions:
        #         - key: beta.kubernetes.io/instance-type
        #           operator: In
        #           # This should be adjusted to the instance type according to your setup
        #           #
        #           values:
        #           - highio
    # Volume Claim settings.
    # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-volume-claim-templates.html
    #
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 5Ti
        # Adjust to your storage class name
        #
        # storageClassName: local-storage
  - name: warm
    count: 0
    config:
      node.roles: ["data_warm"]
      # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
      # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
      # and leave node.store.allow_mmap unset.
      # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
      #
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 16Gi
              cpu: 2
        # Affinity/Anti-affinity settings for controlling the 'spreading' of Elasticsearch
        # pods across existing hosts.
        # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
        # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html#k8s-affinity-options
        #
        # affinity:
        #   nodeAffinity:
        #     requiredDuringSchedulingIgnoredDuringExecution:
        #       nodeSelectorTerms:
        #       - matchExpressions:
        #         - key: beta.kubernetes.io/instance-type
        #           operator: In
        #           # This should be adjusted to the instance type according to your setup
        #           #
        #           values:
        #           - highstorage
    # Volume Claim settings.
    # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-volume-claim-templates.html
    #
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Ti
        # Adjust to your storage class name
        #
        # storageClassName: local-storage
  - name: frozen
    count: 3
    config:
      node.roles: ["data_frozen"]
      # Comment out when setting the vm.max_map_count via initContainer, as these are mutually exclusive.
      # For production workloads, it is strongly recommended to increase the kernel setting vm.max_map_count to 262144
      # and leave node.store.allow_mmap unset.
      # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-virtual-memory.html
      #
      node.store.allow_mmap: false
    podTemplate:
      spec:
        containers:
        - name: elasticsearch
          resources:
            limits:
              memory: 8Gi
              cpu: 2
        # Affinity/Anti-affinity settings for controlling the 'spreading' of Elasticsearch
        # pods across existing hosts.
        # ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
        # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-advanced-node-scheduling.html#k8s-affinity-options
        #
        # affinity:
        #   nodeAffinity:
        #     requiredDuringSchedulingIgnoredDuringExecution:
        #       nodeSelectorTerms:
        #       - matchExpressions:
        #         - key: beta.kubernetes.io/instance-type
        #           operator: In
        #           # This should be adjusted to the instance type according to your setup
        #           #
        #           values:
        #           - highstorage
    # Volume Claim settings.
    # ref: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-volume-claim-templates.html
    #
    volumeClaimTemplates:
    - metadata:
        name: elasticsearch-data
      spec:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 10Ti
        # Adjust to your storage class name
        #
        # storageClassName: local-storage
---
# Version of Kibana.
#
version: 9.0.2

# Labels that will be applied to Kibana.
#
labels: {}
  # key: value

# Annotations that will be applied to Kibana.
#
annotations: {}
  # key: value

# Count of Kibana replicas to create.
#
count: 1

# Reference to ECK-managed Elasticsearch resource, ideally from {{ "elasticsearch.fullname" }}
#
elasticsearchRef:
  name: eck-elasticsearch
  namespace: elastic-stack
http:
  service:
    spec:
      # Type of service to deploy for Kibana.
      # This deploys a load balancer in a cloud service provider, where supported.
      #
      type: LoadBalancer
  tls:
    selfSignedCertificate:
      subjectAltNames:
      - ip: 192.168.10.50
      - ip: 192.168.10.56
      - dns: kibana.lab.thewortmans.org
      - dns: kibana.bwortman.us
config:
  server:
    publicBaseUrl: "https://kibana.bwortman.us"  
